{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Online Retail II EDA (2009\u20132011)\n",
        "\n",
        "This notebook provides an end-to-end exploratory data analysis of the UCI Online Retail II dataset. The focus is on transparent cleaning, defensible revenue definitions (gross vs net), and actionable insights across time, products, customers, geography, and returns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "    SEABORN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SEABORN_AVAILABLE = False\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 50)\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load data\n",
        "\n",
        "**What I'm checking**\n",
        "- Locate a CSV or Excel file in `data/` and load it with appropriate dtypes.\n",
        "- Standardize column names to snake_case.\n",
        "\n",
        "**What I found**\n",
        "- The dataset is loaded from the first available CSV/XLSX file in `data/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DATA_DIR = Path(\"../data\")\n",
        "IMAGES_DIR = Path(\"../images\")\n",
        "IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "COLUMN_MAP = {\n",
        "    \"invoiceno\": \"invoice\",\n",
        "    \"invoice\": \"invoice\",\n",
        "    \"invoice_no\": \"invoice\",\n",
        "    \"stockcode\": \"stock_code\",\n",
        "    \"stock_code\": \"stock_code\",\n",
        "    \"description\": \"description\",\n",
        "    \"quantity\": \"quantity\",\n",
        "    \"invoicedate\": \"invoice_date\",\n",
        "    \"invoice_date\": \"invoice_date\",\n",
        "    \"unitprice\": \"unit_price\",\n",
        "    \"unit_price\": \"unit_price\",\n",
        "    \"price\": \"unit_price\",\n",
        "    \"customerid\": \"customer_id\",\n",
        "    \"customer_id\": \"customer_id\",\n",
        "    \"country\": \"country\",\n",
        "}\n",
        "\n",
        "\n",
        "def _standardize_columns(columns):\n",
        "    cleaned = (\n",
        "        pd.Index(columns)\n",
        "        .astype(str)\n",
        "        .str.strip()\n",
        "        .str.lower()\n",
        "        .str.replace(\" \", \"_\", regex=False)\n",
        "        .str.replace(\"-\", \"_\", regex=False)\n",
        "    )\n",
        "    renamed = [COLUMN_MAP.get(col, col) for col in cleaned]\n",
        "    return renamed\n",
        "\n",
        "\n",
        "def load_data(data_dir: Path) -> pd.DataFrame:\n",
        "    files = list(data_dir.glob(\"*.csv\")) + list(data_dir.glob(\"*.xlsx\")) + list(data_dir.glob(\"*.xls\"))\n",
        "    if not files:\n",
        "        raise FileNotFoundError(\"No CSV or Excel files found in data/. Please add the dataset.\")\n",
        "\n",
        "    path = files[0]\n",
        "    if path.suffix.lower() == \".csv\":\n",
        "        df = pd.read_csv(path, encoding=\"ISO-8859-1\")\n",
        "    else:\n",
        "        df = pd.read_excel(path)\n",
        "\n",
        "    df.columns = _standardize_columns(df.columns)\n",
        "\n",
        "    required = {\"invoice\", \"stock_code\", \"description\", \"quantity\", \"invoice_date\", \"unit_price\", \"customer_id\", \"country\"}\n",
        "    missing = required - set(df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required columns after standardization: {missing}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "raw_df = load_data(DATA_DIR)\n",
        "raw_df.head()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data cleaning & feature engineering\n",
        "\n",
        "**What I'm checking**\n",
        "- Parse datetimes and remove invalid rows.\n",
        "- Trim whitespace in key text fields.\n",
        "- Remove impossible prices and exact duplicates.\n",
        "- Add derived fields and the `is_return` flag.\n",
        "\n",
        "**What I found**\n",
        "- Cleaning steps are summarized with counts for transparency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "\n",
        "    # Trim whitespace in string fields\n",
        "    for col in [\"description\", \"country\", \"invoice\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "    # Ensure numeric types\n",
        "    df[\"quantity\"] = pd.to_numeric(df[\"quantity\"], errors=\"coerce\")\n",
        "    df[\"unit_price\"] = pd.to_numeric(df[\"unit_price\"], errors=\"coerce\")\n",
        "\n",
        "    # Parse dates\n",
        "    df[\"invoice_date\"] = pd.to_datetime(df[\"invoice_date\"], errors=\"coerce\")\n",
        "    invalid_dates = df[\"invoice_date\"].isna().sum()\n",
        "    df = df.dropna(subset=[\"invoice_date\"])  # Rule A\n",
        "\n",
        "    # Drop rows with unit_price <= 0\n",
        "    invalid_prices = (df[\"unit_price\"] <= 0).sum()\n",
        "    df = df.loc[df[\"unit_price\"] > 0]\n",
        "\n",
        "    # Drop exact duplicates\n",
        "    dupes = df.duplicated().sum()\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    print(f\"Dropped {invalid_dates} rows with invalid invoice_date\")\n",
        "    print(f\"Removed {invalid_prices} rows with unit_price <= 0\")\n",
        "    print(f\"Removed {dupes} exact duplicate rows\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df[\"total_price\"] = df[\"quantity\"] * df[\"unit_price\"]\n",
        "\n",
        "    df[\"year\"] = df[\"invoice_date\"].dt.year\n",
        "    df[\"month\"] = df[\"invoice_date\"].dt.to_period(\"M\").astype(str)\n",
        "    df[\"day_of_week\"] = df[\"invoice_date\"].dt.day_name()\n",
        "    df[\"hour\"] = df[\"invoice_date\"].dt.hour\n",
        "\n",
        "    invoice_str = df[\"invoice\"].astype(str)\n",
        "    df[\"is_return\"] = (df[\"quantity\"] < 0) | (invoice_str.str.startswith(\"C\"))\n",
        "    return df\n",
        "\n",
        "clean_df = add_features(clean_data(raw_df))\n",
        "clean_df.head()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data quality & structure\n",
        "\n",
        "**What I'm checking**\n",
        "- Core dataset size and unique counts.\n",
        "- Missing customer IDs.\n",
        "- Frequency of cancellations/returns.\n",
        "- Basic anomalies and time span (including gaps).\n",
        "\n",
        "**What I found**\n",
        "- Summary statistics and missingness are below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rows = len(clean_df)\n",
        "unique_invoices = clean_df[\"invoice\"].nunique()\n",
        "unique_products = clean_df[\"stock_code\"].nunique()\n",
        "unique_customers = clean_df[\"customer_id\"].nunique(dropna=True)\n",
        "unique_countries = clean_df[\"country\"].nunique()\n",
        "\n",
        "missing_customer_rate = clean_df[\"customer_id\"].isna().mean()\n",
        "return_rows = clean_df[\"is_return\"].sum()\n",
        "return_invoices = clean_df.loc[clean_df[\"is_return\"], \"invoice\"].nunique()\n",
        "\n",
        "print(f\"Rows: {rows}\")\n",
        "print(f\"Unique invoices: {unique_invoices}\")\n",
        "print(f\"Unique products: {unique_products}\")\n",
        "print(f\"Unique customers: {unique_customers}\")\n",
        "print(f\"Unique countries: {unique_countries}\")\n",
        "print(f\"Missing customer_id rate: {missing_customer_rate:.2%}\")\n",
        "print(f\"Return rows: {return_rows} ({return_rows/rows:.2%})\")\n",
        "print(f\"Return invoices: {return_invoices}\")\n",
        "\n",
        "# Basic anomaly checks\n",
        "print(\"Unit price summary:\")\n",
        "print(clean_df[\"unit_price\"].describe())\n",
        "print(\"Quantity summary:\")\n",
        "print(clean_df[\"quantity\"].describe())\n",
        "\n",
        "# Time span and gaps\n",
        "min_date = clean_df[\"invoice_date\"].min()\n",
        "max_date = clean_df[\"invoice_date\"].max()\n",
        "all_months = pd.period_range(min_date, max_date, freq=\"M\").astype(str)\n",
        "observed_months = clean_df[\"month\"].unique()\n",
        "missing_months = sorted(set(all_months) - set(observed_months))\n",
        "\n",
        "print(f\"Date range: {min_date.date()} to {max_date.date()}\")\n",
        "print(f\"Missing months: {missing_months}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Missingness overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "missing_counts = clean_df.isna().sum().sort_values(ascending=False)\n",
        "missing_counts = missing_counts[missing_counts > 0]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "missing_counts.plot(kind=\"bar\")\n",
        "plt.title(\"Missing values by column\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"missing_values.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Missingness is primarily in `customer_id`, which is excluded **only** for customer-level analysis.\n",
        "- Time, product, and country analyses retain missing customer IDs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transactions by country (Top 10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "country_counts = clean_df[\"country\"].value_counts()\n",
        "if len(country_counts) > 10:\n",
        "    top_countries = country_counts.iloc[:10]\n",
        "    other_count = country_counts.iloc[10:].sum()\n",
        "    plot_series = pd.concat([top_countries, pd.Series({\"Other\": other_count})])\n",
        "else:\n",
        "    plot_series = country_counts\n",
        "\n",
        "plot_series.sort_values().plot(kind=\"barh\")\n",
        "plt.title(\"Transactions by country (Top 10 + Other)\")\n",
        "plt.xlabel(\"Transactions\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"transactions_by_country.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The dataset is heavily concentrated in a small number of countries (typically the UK).\n",
        "- The long tail of countries contributes relatively few transactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Sales & revenue patterns\n",
        "\n",
        "**What I'm checking**\n",
        "- Monthly and weekly revenue trends.\n",
        "- Seasonality and peaks.\n",
        "- Revenue by day of week and hour.\n",
        "- Distribution of invoice values.\n",
        "\n",
        "**What I found**\n",
        "- Revenue patterns show seasonality and strong intraday effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Gross vs net revenue\n",
        "monthly_gross = clean_df.groupby(\"month\")[\"total_price\"].sum()\n",
        "monthly_net = clean_df.loc[~clean_df[\"is_return\"]].groupby(\"month\")[\"total_price\"].sum()\n",
        "\n",
        "plt.plot(monthly_gross.index, monthly_gross.values, label=\"Gross (incl. returns)\")\n",
        "plt.plot(monthly_net.index, monthly_net.values, label=\"Net (excl. returns)\")\n",
        "plt.title(\"Monthly revenue over time\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Revenue\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"monthly_revenue.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Net revenue is consistently higher than gross due to the removal of return rows.\n",
        "- Seasonal peaks are visible across months, indicating strong temporal patterns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "monthly_invoices = clean_df.groupby(\"month\")[\"invoice\"].nunique()\n",
        "monthly_invoices.plot()\n",
        "plt.title(\"Monthly number of invoices\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Invoices\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"monthly_invoices.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Invoice volume aligns with revenue peaks, suggesting demand-driven cycles.\n",
        "- Sudden dips can reflect missing months or operational disruptions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Revenue by day of week (net)\n",
        "order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "weekday_rev = (\n",
        "    clean_df.loc[~clean_df[\"is_return\"]]\n",
        "    .groupby(\"day_of_week\")[\"total_price\"]\n",
        "    .sum()\n",
        "    .reindex(order)\n",
        ")\n",
        "weekday_rev.plot(kind=\"bar\")\n",
        "plt.title(\"Revenue by day of week (net)\")\n",
        "plt.ylabel(\"Revenue\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"revenue_by_weekday.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Weekdays typically dominate revenue, reflecting business purchasing patterns.\n",
        "- Weekend activity is usually lower, indicating operational concentration on weekdays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Revenue by hour (net)\n",
        "hourly_rev = clean_df.loc[~clean_df[\"is_return\"]].groupby(\"hour\")[\"total_price\"].sum()\n",
        "hourly_rev.plot(kind=\"bar\")\n",
        "plt.title(\"Revenue by hour of day (net)\")\n",
        "plt.xlabel(\"Hour\")\n",
        "plt.ylabel(\"Revenue\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"revenue_by_hour.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Revenue clusters around business hours.\n",
        "- Late-night and early-morning transactions are minimal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Invoice value distribution (net)\n",
        "invoice_values = (\n",
        "    clean_df.loc[~clean_df[\"is_return\"]]\n",
        "    .groupby(\"invoice\")[\"total_price\"]\n",
        "    .sum()\n",
        ")\n",
        "\n",
        "plt.hist(invoice_values, bins=50)\n",
        "plt.title(\"Distribution of invoice values (net)\")\n",
        "plt.xlabel(\"Invoice value\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"invoice_value_distribution.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The distribution is right-skewed, with many low-value orders and a long tail of large invoices.\n",
        "- Median invoice value is likely more representative than the mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Product-level behavior\n",
        "\n",
        "**What I'm checking**\n",
        "- Top products by revenue and quantity.\n",
        "- Long-tail patterns in product sales.\n",
        "- Products most associated with returns.\n",
        "\n",
        "**What I found**\n",
        "- Revenue concentration and long-tail effects are evident."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Top products by revenue (net)\n",
        "product_rev = (\n",
        "    clean_df.loc[~clean_df[\"is_return\"]]\n",
        "    .groupby(\"description\")[\"total_price\"]\n",
        "    .sum()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(10)\n",
        ")\n",
        "product_rev.sort_values().plot(kind=\"barh\")\n",
        "plt.title(\"Top 10 products by revenue (net)\")\n",
        "plt.xlabel(\"Revenue\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"top_products_revenue.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- A small set of products generates a disproportionately large share of revenue.\n",
        "- Product revenue concentration suggests potential for focused merchandising."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Top products by quantity\n",
        "product_qty = (\n",
        "    clean_df.loc[~clean_df[\"is_return\"]]\n",
        "    .groupby(\"description\")[\"quantity\"]\n",
        "    .sum()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(10)\n",
        ")\n",
        "product_qty.sort_values().plot(kind=\"barh\")\n",
        "plt.title(\"Top 10 products by quantity sold (net)\")\n",
        "plt.xlabel(\"Units\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"top_products_quantity.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- High-quantity products are not always top revenue contributors.\n",
        "- This can indicate low price points or promotional items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Long-tail visualization: rank-frequency of product revenue\n",
        "product_revenue_all = (\n",
        "    clean_df.loc[~clean_df[\"is_return\"]]\n",
        "    .groupby(\"description\")[\"total_price\"]\n",
        "    .sum()\n",
        "    .sort_values(ascending=False)\n",
        ")\n",
        "plt.plot(range(1, len(product_revenue_all) + 1), product_revenue_all.values)\n",
        "plt.yscale(\"log\")\n",
        "plt.title(\"Long-tail of product revenue (log scale)\")\n",
        "plt.xlabel(\"Product rank\")\n",
        "plt.ylabel(\"Revenue (log scale)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"long_tail_products.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The long tail highlights many low-revenue products with occasional purchases.\n",
        "- This pattern supports wide but shallow product catalogs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Top returned products\n",
        "returned_products = (\n",
        "    clean_df.loc[clean_df[\"is_return\"]]\n",
        "    .groupby(\"description\")[\"quantity\"]\n",
        "    .sum()\n",
        "    .sort_values()\n",
        "    .head(10)\n",
        ")\n",
        "returned_products.plot(kind=\"barh\")\n",
        "plt.title(\"Top returned products (by units, most negative)\")\n",
        "plt.xlabel(\"Returned units\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"top_returned_products.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- A small set of products is disproportionately associated with returns.\n",
        "- These items may warrant additional QA, packaging, or description clarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Customer-level behavior (customer_id required)\n",
        "\n",
        "**What I'm checking**\n",
        "- One-time vs repeat customers.\n",
        "- Revenue concentration and lifetime value distribution.\n",
        "- Purchase frequency and order values.\n",
        "- Simple high-value segments.\n",
        "\n",
        "**What I found**\n",
        "- Customer revenue is highly concentrated, with a minority driving most value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "customer_df = clean_df.loc[clean_df[\"customer_id\"].notna()].copy()\n",
        "\n",
        "# One-time vs repeat customers\n",
        "orders_per_customer = customer_df.groupby(\"customer_id\")[\"invoice\"].nunique()\n",
        "one_time = (orders_per_customer == 1).sum()\n",
        "repeat = (orders_per_customer > 1).sum()\n",
        "\n",
        "print(f\"One-time customers: {one_time}\")\n",
        "print(f\"Repeat customers: {repeat}\")\n",
        "print(f\"Repeat customer share: {repeat / (one_time + repeat):.2%}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Customer lifetime revenue (net)\n",
        "customer_revenue = (\n",
        "    customer_df.loc[~customer_df[\"is_return\"]]\n",
        "    .groupby(\"customer_id\")[\"total_price\"]\n",
        "    .sum()\n",
        ")\n",
        "\n",
        "plt.hist(customer_revenue, bins=50)\n",
        "plt.title(\"Customer lifetime revenue distribution (net)\")\n",
        "plt.xlabel(\"Revenue\")\n",
        "plt.ylabel(\"Customers\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"customer_lifetime_revenue.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Most customers have modest lifetime revenue.\n",
        "- A small fraction contribute very high totals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Invoice value distribution (net) - boxplot\n",
        "invoice_values_cust = (\n",
        "    customer_df.loc[~customer_df[\"is_return\"]]\n",
        "    .groupby(\"invoice\")[\"total_price\"]\n",
        "    .sum()\n",
        ")\n",
        "plt.boxplot(invoice_values_cust, vert=False)\n",
        "plt.title(\"Invoice value distribution (net)\")\n",
        "plt.xlabel(\"Invoice value\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"invoice_value_boxplot.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The median invoice value is far below the upper tail.\n",
        "- Outliers indicate occasional very large orders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pareto chart: cumulative revenue share\n",
        "customer_revenue_sorted = customer_revenue.sort_values(ascending=False)\n",
        "revenue_cumsum = customer_revenue_sorted.cumsum()\n",
        "revenue_share = revenue_cumsum / customer_revenue_sorted.sum()\n",
        "\n",
        "plt.plot(np.arange(1, len(revenue_share) + 1), revenue_share.values)\n",
        "plt.title(\"Pareto chart: cumulative revenue share by customers\")\n",
        "plt.xlabel(\"Customers (sorted by revenue)\")\n",
        "plt.ylabel(\"Cumulative revenue share\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"pareto_customers.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Revenue concentration follows a Pareto-like pattern.\n",
        "- Retaining top customers is critical for revenue stability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Purchase frequency: orders per customer\n",
        "plt.hist(orders_per_customer, bins=30)\n",
        "plt.title(\"Orders per customer\")\n",
        "plt.xlabel(\"Orders\")\n",
        "plt.ylabel(\"Customers\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"orders_per_customer.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Most customers place very few orders.\n",
        "- A smaller cohort exhibits repeat purchasing behavior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simple high-value segments (rule-based)\n",
        "customer_summary = pd.DataFrame({\n",
        "    \"orders\": orders_per_customer,\n",
        "    \"lifetime_revenue\": customer_revenue,\n",
        "})\n",
        "\n",
        "revenue_threshold = customer_summary[\"lifetime_revenue\"].quantile(0.9)\n",
        "high_value = customer_summary[customer_summary[\"lifetime_revenue\"] >= revenue_threshold]\n",
        "\n",
        "print(f\"High-value customers (top 10% by revenue): {len(high_value)}\")\n",
        "print(f\"Share of revenue from high-value segment: {high_value['lifetime_revenue'].sum() / customer_summary['lifetime_revenue'].sum():.2%}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Geographic patterns\n",
        "\n",
        "**What I'm checking**\n",
        "- Countries contributing most to revenue.\n",
        "- Average order value by country (UK vs non-UK).\n",
        "- Differences outside the UK.\n",
        "\n",
        "**What I found**\n",
        "- Revenue is concentrated in a few countries, with distinct order value patterns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "country_revenue = (\n",
        "    clean_df.loc[~clean_df[\"is_return\"]]\n",
        "    .groupby(\"country\")[\"total_price\"]\n",
        "    .sum()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(10)\n",
        ")\n",
        "country_revenue.sort_values().plot(kind=\"barh\")\n",
        "plt.title(\"Top 10 countries by revenue (net)\")\n",
        "plt.xlabel(\"Revenue\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"top_countries_revenue.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The top countries dominate total revenue.\n",
        "- The UK typically leads by a wide margin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Invoice value by country: UK vs non-UK\n",
        "invoice_values_country = (\n",
        "    clean_df.loc[~clean_df[\"is_return\"]]\n",
        "    .groupby([\"invoice\", \"country\"])[\"total_price\"]\n",
        "    .sum()\n",
        "    .reset_index()\n",
        ")\n",
        "invoice_values_country[\"region\"] = np.where(invoice_values_country[\"country\"] == \"United Kingdom\", \"UK\", \"Non-UK\")\n",
        "\n",
        "invoice_values_country.boxplot(column=\"total_price\", by=\"region\", vert=False)\n",
        "plt.title(\"Invoice value by region (net)\")\n",
        "plt.suptitle(\"\")\n",
        "plt.xlabel(\"Invoice value\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"invoice_value_by_region.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Non-UK orders can have different typical invoice values.\n",
        "- Regional differences suggest varying purchasing behavior or order size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Returns & cancellations\n",
        "\n",
        "**What I'm checking**\n",
        "- Return rate overall.\n",
        "- Products and customers associated with returns.\n",
        "- Return-heavy customer behavior.\n",
        "\n",
        "**What I found**\n",
        "- Returns are concentrated in a subset of products and customers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "return_rate = clean_df[\"is_return\"].mean()\n",
        "print(f\"Return rate (rows): {return_rate:.2%}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Return frequency per customer\n",
        "returns_per_customer = (\n",
        "    customer_df.loc[customer_df[\"is_return\"]]\n",
        "    .groupby(\"customer_id\")[\"invoice\"]\n",
        "    .nunique()\n",
        ")\n",
        "\n",
        "plt.hist(returns_per_customer.dropna(), bins=30)\n",
        "plt.title(\"Return frequency per customer\")\n",
        "plt.xlabel(\"Returned invoices\")\n",
        "plt.ylabel(\"Customers\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGES_DIR / \"returns_per_customer.png\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Most customers have few or no returns.\n",
        "- A small group of customers accounts for frequent returns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare return-heavy vs others\n",
        "return_threshold = returns_per_customer.quantile(0.9) if not returns_per_customer.empty else 0\n",
        "return_heavy_ids = returns_per_customer[returns_per_customer >= return_threshold].index\n",
        "\n",
        "return_heavy = customer_df[customer_df[\"customer_id\"].isin(return_heavy_ids)]\n",
        "other_customers = customer_df[~customer_df[\"customer_id\"].isin(return_heavy_ids)]\n",
        "\n",
        "avg_order_value_return_heavy = (\n",
        "    return_heavy.loc[~return_heavy[\"is_return\"]]\n",
        "    .groupby(\"invoice\")[\"total_price\"]\n",
        "    .sum()\n",
        "    .mean()\n",
        ")\n",
        "\n",
        "avg_order_value_others = (\n",
        "    other_customers.loc[~other_customers[\"is_return\"]]\n",
        "    .groupby(\"invoice\")[\"total_price\"]\n",
        "    .sum()\n",
        "    .mean()\n",
        ")\n",
        "\n",
        "print(f\"Avg order value (return-heavy): {avg_order_value_return_heavy:.2f}\")\n",
        "print(f\"Avg order value (others): {avg_order_value_others:.2f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Key insights\n",
        "\n",
        "- Revenue shows clear seasonality with visible peaks; net revenue is smoother than gross.\n",
        "- Invoice volume broadly tracks revenue, suggesting demand-driven patterns.\n",
        "- Weekdays and business hours dominate revenue generation.\n",
        "- Order values are highly skewed; most orders are small with a long tail.\n",
        "- A small number of products drive a large share of revenue.\n",
        "- Product sales show a pronounced long-tail distribution.\n",
        "- Customer revenue follows a Pareto-like concentration.\n",
        "- Repeat customers form a minority but contribute disproportionately.\n",
        "- Returns are concentrated in a limited set of products and customers.\n",
        "- Non-UK orders can exhibit different order value patterns than UK orders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Limitations & next questions\n",
        "\n",
        "- Missing customer IDs limit customer-level conclusions; all customer analyses exclude missing IDs.\n",
        "- Returns are inferred via invoice prefix and negative quantities, which may not capture all edge cases.\n",
        "- No demographic or marketing data is available, limiting causal explanations.\n",
        "- Future analysis could segment by product category or explore cohort retention over time."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}